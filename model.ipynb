{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "fname='MSCI446_ Data - Sheet1.csv'\n",
    "\n",
    "def convert_to_numpy_array(string: str):\n",
    "    # Verify string is written like a python list\n",
    "    stripped = string.strip()\n",
    "    pattern = r\"\\[(.*)\\]\"\n",
    "    match = re.match(pattern, stripped)\n",
    "    if not match:\n",
    "        raise NotImplementedError(f\"{stripped}\")\n",
    "    # Group 1 captures the content inside the square brackets\n",
    "    contents = match.group(1)\n",
    "    \n",
    "    try:\n",
    "        # Use ast.literal_eval to safely evaluate string literals\n",
    "        evaluated = ast.literal_eval(stripped)\n",
    "        if type(evaluated) is not list:\n",
    "            raise NotImplementedError()\n",
    "        # series = pd.Series(evaluated)\n",
    "        return np.array(evaluated)\n",
    "    except (SyntaxError, ValueError):  # this occurs with the \"Condition\" column\n",
    "        # If parsing as a list fails, split the contents within the square brackets by comma, \n",
    "        # interpret each value as a string and strip whitespace\n",
    "        return np.array([ item.strip() for item in contents.split(',') ])\n",
    "\n",
    "# Read CSV file with the custom function\n",
    "numerical_vector_column_labels = ['Temperature (F)', 'Dewpoint (F)', 'Humidity (%)', 'Wind Speed (mph)', 'Pressure (in)', 'Percipitation (in)']\n",
    "categorical_vector_column_labels = [\"Condition\"]\n",
    "vector_column_labels = numerical_vector_column_labels + categorical_vector_column_labels\n",
    "\n",
    "# Create a converters dictionary mapping each column to the converter function\n",
    "converters = {col: convert_to_numpy_array for col in vector_column_labels}\n",
    "df = pd.read_csv(fname, converters=converters)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Data Ain't Trash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_column_labels = [\"Season Started\", \"Locations Affected\", \"Condition\", \"Type Of Location\", \"Y-Value\"]\n",
    "categorical_scalar_column_labels = [\"Season Started\", \"Locations Affected\", \"Type Of Location\", \"Y-Value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in categorical_scalar_column_labels:\n",
    "    print(df[cl].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_columns = df[vector_column_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying all are ndarray\n",
    "is_ndarray = vector_columns.map(lambda x: isinstance(x, np.ndarray))\n",
    "is_ndarray.all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_shape = (12,)\n",
    "\n",
    "shape = vector_columns.map(lambda x: x.shape)\n",
    "# shape == desired_shape  # INVALID SYNTAX\n",
    "is_desired_shape = shape.map(lambda x: x == desired_shape)\n",
    "\n",
    "# Shorter version\n",
    "is_desired_shape = vector_columns.map(lambda x: x.shape == desired_shape)\n",
    "\n",
    "# Boolean series containing whether all vectors in that row are of the desired shape\n",
    "valid_shape_rows = is_desired_shape.all(axis=1)\n",
    "\n",
    "# Whether all vectors are of desired shape\n",
    "is_desired_shape.all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate minimum, average, and maximum values for specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to generate stats for\n",
    "columns_to_process = numerical_vector_column_labels\n",
    "\n",
    "# Function to compute min, avg, and max and return as a Series\n",
    "def compute_stats(arr):\n",
    "    return pd.Series([np.min(arr), np.mean(arr), np.max(arr)], index=['min', 'avg', 'max'])\n",
    "\n",
    "# New DataFrame to store results\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over specified columns\n",
    "for col in df.columns:\n",
    "    # If the column contains numpy arrays and is in the columns to process\n",
    "    if col in columns_to_process:  # and np.issubdtype(df[col].dtype, np.ndarray)\n",
    "        # Compute statistics for each numpy array element in the column\n",
    "        stats = df[col].apply(compute_stats)\n",
    "        # Rename columns to include the statistics\n",
    "        stats.columns = [f\"{col}_min\", f\"{col}_avg\", f\"{col}_max\"]\n",
    "        # Concatenate the statistics columns with the original column and insert them into the new DataFrame\n",
    "        new_df = pd.concat([new_df, df[col], stats], axis=1)\n",
    "    else:\n",
    "        # If not a numpy array column or not in columns to process, copy it to the new DataFrame\n",
    "        new_df[col] = df[col]\n",
    "\n",
    "# print(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking element values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select for columns where we care if values are missing (do not care if we are missing Fire Name)\n",
    "isna = df.loc[:, \"Year\":\"Y-Value\"].isna()\n",
    "isna.sum()[isna.any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking contents for numpy arrays (numerical only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not df[numerical_vector_column_labels].map(lambda x: np.any(np.isnan(x))).any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dealing with duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dealing with categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns containing single categorical values\n",
    "for cl in categorical_scalar_column_labels:\n",
    "    df[cl] = le.fit_transform(df[cl])\n",
    "\n",
    "# Columns containing categorical vector values\n",
    "for cl in categorical_vector_column_labels:\n",
    "    le.fit(np.concatenate(df['Condition']))\n",
    "    df[cl] = df[cl].apply(le.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dealing with Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Partitioning a data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
